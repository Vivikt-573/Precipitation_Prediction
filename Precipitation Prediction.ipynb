{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## _Table of Contents üìã_\n1. [Importing Libraries](#CH-1)\n2. [Data Importing](#CH-2)\n    * Data Quality\n        * Handling Duplicate Rows and Columns\n        * Missing Values\n        * Uniqueness\n    * Basic Modeification on DataFrame\n    * Data Describing\n    * Dataset Description\n    * Feature Definition\n3. [Exploratory Data Analysis](#CH-3)\n4. [Feature Engineering](#CH-4)\n    * Train-Test Split\n    * Handling Missing Values\n    * Outiler Treatment\n    * Feature Encoding\n5. [Decision Tree](#CH-5)\n    * Building Model + Cross Validation + Hyper-Parameter Tuning\n    * Model Evaluation\n6. [Random Forest](#CH-6)\n    * Building Model + Cross Validation + Hyper-Parameter Tuning\n    * Model Evaluation\n7. [Ada Boost](#CH-7)\n    * Building Model + Cross Validation + Hyper-Parameter Tuning\n    * Model Evaluation\n8. [Gradient Boost](#CH-8)\n    * Building Model + Cross Validation + Hyper-Parameter Tuning\n    * Model Evaluation\n9. [XG Boost](#CH-9)\n    * Building Model + Cross Validation + Hyper-Parameter Tuning\n    * Model Evaluation\n10. [Result and Discussion](#CH-10)","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:white;\n           display:fill;\n           border-radius:20px;\n           background-color:#00F180;\n           font-size:200%;\n           font-family:Verdana;\n           letter-spacing:1px\">\n<a class=\"anchor\" id=\"CH-1\"></a> \n<p style=\"padding: 10px;\n              color:white;\">\n              1. Importing Libraries üìö\n</p>\n</div>","metadata":{}},{"cell_type":"code","source":"# Importing Basic Libraries\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\",category=FutureWarning)\n\n# Other Essential Libraries will be Imported As and When Required","metadata":{"execution":{"iopub.status.busy":"2023-10-22T14:55:06.255956Z","iopub.execute_input":"2023-10-22T14:55:06.256596Z","iopub.status.idle":"2023-10-22T14:55:08.577065Z","shell.execute_reply.started":"2023-10-22T14:55:06.256512Z","shell.execute_reply":"2023-10-22T14:55:08.575761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;\n           display:fill;\n           border-radius:20px;\n           background-color:#00F180;\n           font-size:200%;\n           font-family:Verdana;\n           letter-spacing:1px\">\n<a class=\"anchor\" id=\"CH-2\"></a> \n<p style=\"padding: 10px;\n              color:white;\">\n              2. Data Import and Auditing üë®‚Äçüíª\n</p>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<h2 align=\"left\"><font color='#0E9D00'><u>Data Import</u></font></h2>","metadata":{}},{"cell_type":"code","source":"# Data Importing from Kaggle\ndf = pd.read_csv(\"/kaggle/input/weather-dataset-rattle-package/weatherAUS.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-10-22T14:55:08.579239Z","iopub.execute_input":"2023-10-22T14:55:08.580529Z","iopub.status.idle":"2023-10-22T14:55:09.581260Z","shell.execute_reply.started":"2023-10-22T14:55:08.580485Z","shell.execute_reply":"2023-10-22T14:55:09.579902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dimension of DataFrame\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2023-10-22T14:55:09.583242Z","iopub.execute_input":"2023-10-22T14:55:09.584086Z","iopub.status.idle":"2023-10-22T14:55:09.594287Z","shell.execute_reply.started":"2023-10-22T14:55:09.584031Z","shell.execute_reply":"2023-10-22T14:55:09.592877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 align=\"left\"><font color='#0E9D00'><u>Data View</u></font></h2>","metadata":{}},{"cell_type":"code","source":"# Data Viewing Initial 11 columns\ndf.iloc[:5,:11]","metadata":{"execution":{"iopub.status.busy":"2023-10-22T14:55:09.599618Z","iopub.execute_input":"2023-10-22T14:55:09.600655Z","iopub.status.idle":"2023-10-22T14:55:09.650544Z","shell.execute_reply.started":"2023-10-22T14:55:09.600608Z","shell.execute_reply":"2023-10-22T14:55:09.648851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data Viewing Rest of the columns\ndf.iloc[:5,11:]","metadata":{"execution":{"iopub.status.busy":"2023-10-22T14:55:09.652992Z","iopub.execute_input":"2023-10-22T14:55:09.653577Z","iopub.status.idle":"2023-10-22T14:55:09.684345Z","shell.execute_reply.started":"2023-10-22T14:55:09.653504Z","shell.execute_reply":"2023-10-22T14:55:09.683282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Understanding Data - Total Entries, Number of Columns, Datatype\ndf.info()","metadata":{"execution":{"iopub.status.busy":"2023-10-22T14:55:09.685699Z","iopub.execute_input":"2023-10-22T14:55:09.686627Z","iopub.status.idle":"2023-10-22T14:55:09.853548Z","shell.execute_reply.started":"2023-10-22T14:55:09.686571Z","shell.execute_reply":"2023-10-22T14:55:09.852168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 align=\"left\"><font color='#0E9D00'><u>Data Quality</u></font></h2>","metadata":{}},{"cell_type":"code","source":"# Duplicate Rows\ndf.duplicated().sum()","metadata":{"execution":{"iopub.status.busy":"2023-10-22T14:55:09.855789Z","iopub.execute_input":"2023-10-22T14:55:09.856661Z","iopub.status.idle":"2023-10-22T14:55:10.169619Z","shell.execute_reply.started":"2023-10-22T14:55:09.856604Z","shell.execute_reply":"2023-10-22T14:55:10.168202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Missing_values = pd.DataFrame((df.isnull().sum()/len(df)*100).sort_values(ascending=False))\n\nax=plt.figure(figsize=(10,5))\nax = sns.barplot(data=Missing_values,y=Missing_values.index,x=0)\nfor i in ax.containers:\n    ax.bar_label(i, fmt='%.2f', label_type='edge')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-22T14:55:10.171515Z","iopub.execute_input":"2023-10-22T14:55:10.171897Z","iopub.status.idle":"2023-10-22T14:55:10.933709Z","shell.execute_reply.started":"2023-10-22T14:55:10.171864Z","shell.execute_reply":"2023-10-22T14:55:10.932346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Features Having Missing Value above - 10 % will be Dropped\ndf.drop(['Sunshine','Evaporation','Cloud3pm','Cloud9am'],axis=1,inplace=True)\n\n# Dropping Unwanted Columns\ndf.drop(['Date'],axis=1,inplace=True)\n\n# Dropping Null Values From Target Variable - 2.25%\ndf.dropna(subset=['RainTomorrow'], axis=0, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-10-22T14:55:10.935202Z","iopub.execute_input":"2023-10-22T14:55:10.936190Z","iopub.status.idle":"2023-10-22T14:55:11.040127Z","shell.execute_reply.started":"2023-10-22T14:55:10.936134Z","shell.execute_reply":"2023-10-22T14:55:11.039120Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Unique Values in Each Feature\ndistinct_counts = []\n\n\nfor column in df.columns:\n    distinct_count = df[column].nunique()  \n    first_5_unique_values = df[column].unique()[:5]  \n    last_5_unique_values = df[column].unique()[-5:] \n    distinct_counts.append({\n        'Column': column, \n        'Distinct_Values_Count': distinct_count, \n        'First_5_Unique_Values': first_5_unique_values,\n        'Last_5_Unique_Values': last_5_unique_values\n    })\n\ndistinct_counts_df = pd.DataFrame(distinct_counts)\n\n\ndistinct_counts_df.sort_values(by='Distinct_Values_Count', ascending=False,ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2023-10-22T14:55:11.044306Z","iopub.execute_input":"2023-10-22T14:55:11.045957Z","iopub.status.idle":"2023-10-22T14:55:11.463716Z","shell.execute_reply.started":"2023-10-22T14:55:11.045901Z","shell.execute_reply":"2023-10-22T14:55:11.462443Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 align=\"left\"><font color='#0E9D00'><u>Data Description</u></font></h2>","metadata":{}},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2023-10-22T14:55:11.465441Z","iopub.execute_input":"2023-10-22T14:55:11.465950Z","iopub.status.idle":"2023-10-22T14:55:11.641039Z","shell.execute_reply.started":"2023-10-22T14:55:11.465901Z","shell.execute_reply":"2023-10-22T14:55:11.639851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 align=\"left\"><font color='#0E9D00'><u>Data Definition</u></font></h2>","metadata":{}},{"cell_type":"code","source":"# Data Defining for Visualization and Processing\nnumeric_columns = df.select_dtypes(include=np.number).columns.tolist()\ncategorical_columns = df.select_dtypes(exclude=np.number).columns.tolist()\ncategorical_columns.remove('RainTomorrow')\nnumeric_columns,categorical_columns","metadata":{"execution":{"iopub.status.busy":"2023-10-22T14:55:11.642790Z","iopub.execute_input":"2023-10-22T14:55:11.643573Z","iopub.status.idle":"2023-10-22T14:55:11.666427Z","shell.execute_reply.started":"2023-10-22T14:55:11.643507Z","shell.execute_reply":"2023-10-22T14:55:11.664676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-success\" style=\"margin-bottom: 20px; font-size: 14px;\">  \n    <b>üìäObservation and Insights:</b>\n    <ul>\n        <li>In total Dataset Comprises 23 features.</li>\n        <li>Target Variable - Rain Tomorrow is Binary Category.</li>\n        <li>Features such as 'Sunshine','Evaporation','Cloud3pm','Cloud9am' have missing values about 40% and hence they were dropped.</li>\n        <li>Statistical Value suggest Outlier's Present in Dataset </li>\n        <li>Rainfall\" feature, 75% - 0.8 and max - 371.0 suggesting a unsual values, meaning it might have some extreme values.</li>\n    </ul>\n</div>","metadata":{"execution":{"iopub.status.busy":"2023-10-19T13:07:02.170010Z","iopub.execute_input":"2023-10-19T13:07:02.170359Z","iopub.status.idle":"2023-10-19T13:07:02.197947Z","shell.execute_reply.started":"2023-10-19T13:07:02.170333Z","shell.execute_reply":"2023-10-19T13:07:02.196957Z"}}},{"cell_type":"markdown","source":"<div style=\"color:white;\n           display:fill;\n           border-radius:20px;\n           background-color:#00F180;\n           font-size:200%;\n           font-family:Verdana;\n           letter-spacing:1px\">\n<a class=\"anchor\" id=\"CH-3\"></a> \n<p style=\"padding: 10px;\n              color:white;\">\n              3. Exploratory Data Analysis (Modelling Perspective‚öôÔ∏è) üïµüèª‚Äç‚ôÇÔ∏è\n</p>\n</div>","metadata":{}},{"cell_type":"code","source":"# Combined Histogram of All Features\ndf[numeric_columns].hist(figsize=(15, 15), edgecolor='r', color='skyblue')\nplt.suptitle('Histogram of Numerical Variables ', fontsize=18)\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-22T14:55:11.668689Z","iopub.execute_input":"2023-10-22T14:55:11.669193Z","iopub.status.idle":"2023-10-22T14:55:15.470109Z","shell.execute_reply.started":"2023-10-22T14:55:11.669147Z","shell.execute_reply":"2023-10-22T14:55:15.468883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Combined Box Plot for All Features\nfig,axes = plt.subplots(6,2,figsize=(15,15))\n\n# Creating a Function for Plots\ndef plot_two(feat,i,j):\n    sns.boxplot(x=df[feat],color='yellow', width=0.5, flierprops={'marker': 'o', 'markerfacecolor': 'red', 'markersize': 3.5}, ax=axes[i,j])\n    axes[i, j].set_title(feat)\n    axes[i, j].set_xlabel('')\n\nfor i,feat in enumerate(numeric_columns):\n    j = i%2 #0 or 1\n    plot_two(feat,i//2,j)\n    warnings.filterwarnings(\"ignore\", category=UserWarning)\n\nfig.suptitle('Box Plots for Numerical Variables', fontsize=16)\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-22T14:55:15.471717Z","iopub.execute_input":"2023-10-22T14:55:15.472358Z","iopub.status.idle":"2023-10-22T14:55:17.651981Z","shell.execute_reply.started":"2023-10-22T14:55:15.472316Z","shell.execute_reply":"2023-10-22T14:55:17.650651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-success\" style=\"margin-bottom: 20px; font-size: 14px;\">  \n    <b>üìäObservation and Insights:</b>\n    <ul>\n        <li>The follwing feature exhibhit Gaussian Distribution 'MinTemp','MaxTemp','Humidity3pm','Pressure9am','Pressure3pm','Temp9am','Temp3pm'</li>\n        <li>The follwing feature exhibhit Non-Gaussian Distribution 'WindGustSpeed','WindSpeed9am','WindSpeed3pm','Humidity9am'.</li>\n        <li>It is quite Evident from the boxplot that the data set has outlier's.</li>\n         <li>As per the Objective of the Study is to Apply Models with Tree based, Which has minimal effect of Feature Engineering - Feature Scaling, Feature Transformation. For this particular kernel More focus will be on trying out Decison Based Algorithm and Extensive Hyper-parameter Tuning .</li>\n    </ul>\n</div>","metadata":{"execution":{"iopub.status.busy":"2023-10-19T13:07:02.170010Z","iopub.execute_input":"2023-10-19T13:07:02.170359Z","iopub.status.idle":"2023-10-19T13:07:02.197947Z","shell.execute_reply.started":"2023-10-19T13:07:02.170333Z","shell.execute_reply":"2023-10-19T13:07:02.196957Z"}}},{"cell_type":"code","source":"sns.pairplot( data=df, vars=numeric_columns, hue='RainTomorrow' )","metadata":{"execution":{"iopub.status.busy":"2023-10-22T14:55:17.653696Z","iopub.execute_input":"2023-10-22T14:55:17.654195Z","iopub.status.idle":"2023-10-22T15:23:58.653929Z","shell.execute_reply.started":"2023-10-22T14:55:17.654157Z","shell.execute_reply":"2023-10-22T15:23:58.651475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 align=\"left\"><font color=#0E9D00><u>Train Test Split -</u> Executing Initially To Prevent Data Leakage</font></h2>","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Train-Test Split\nX_train,X_test,y_train,y_test = train_test_split(df.iloc[:,:17],df.iloc[:,-1],test_size=0.3, random_state=52)\n\n# Converting y_train, y_test back to DataFrame\ny_train = pd.DataFrame(y_train)\ny_test = pd.DataFrame(y_test)\n\nX_train.shape,y_train.shape,X_test.shape,y_test.shape","metadata":{"execution":{"iopub.status.busy":"2023-10-22T15:23:58.657047Z","iopub.execute_input":"2023-10-22T15:23:58.657830Z","iopub.status.idle":"2023-10-22T15:23:59.017192Z","shell.execute_reply.started":"2023-10-22T15:23:58.657768Z","shell.execute_reply":"2023-10-22T15:23:59.015878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-success\" style=\"margin-bottom: 20px; font-size: 14px;\">  \n    <b>üìäObservation and Insights:</b>\n    <ul>\n        <li> By splitting the data first, to ensure that feature engineering is based solely on the training data, to avoid data leakage and making the model evaluation more reliable and realistic.</li>\n    </ul>\n</div>","metadata":{"execution":{"iopub.status.busy":"2023-10-19T13:07:02.170010Z","iopub.execute_input":"2023-10-19T13:07:02.170359Z","iopub.status.idle":"2023-10-19T13:07:02.197947Z","shell.execute_reply.started":"2023-10-19T13:07:02.170333Z","shell.execute_reply":"2023-10-19T13:07:02.196957Z"}}},{"cell_type":"markdown","source":"<div style=\"color:white;\n           display:fill;\n           border-radius:20px;\n           background-color:#00F180;\n           font-size:200%;\n           font-family:Verdana;\n           letter-spacing:1px\">\n<a class=\"anchor\" id=\"CH-4\"></a> \n<p style=\"padding: 10px;\n              color:white;\">\n              4. Feature Engineering üõ†Ô∏è\n</p>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<h2 align=\"left\"><font color=#0E9D00><u>1. Handling Missing Values</u></font></h2>","metadata":{}},{"cell_type":"code","source":"from sklearn.impute import SimpleImputer\n\n# Create a SimpleImputer with the strategy set to 'median'\nmedian_imputer = SimpleImputer(strategy='median')\n\n# Loop through numeric columns and impute missing values with medians\nfor col in numeric_columns:\n    X_train[[col]] = median_imputer.fit_transform(X_train[[col]])\n    X_test[[col]] = median_imputer.fit_transform(X_test[[col]])","metadata":{"execution":{"iopub.status.busy":"2023-10-22T15:23:59.018789Z","iopub.execute_input":"2023-10-22T15:23:59.019177Z","iopub.status.idle":"2023-10-22T15:23:59.631728Z","shell.execute_reply.started":"2023-10-22T15:23:59.019146Z","shell.execute_reply":"2023-10-22T15:23:59.630262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mode_imputer = SimpleImputer(strategy='most_frequent')\n\n# Loop through categorical columns and impute missing values with the mode\nfor col in categorical_columns:\n    X_train[[col]] = mode_imputer.fit_transform(X_train[[col]])\n    X_test[[col]] = mode_imputer.transform(X_test[[col]])","metadata":{"execution":{"iopub.status.busy":"2023-10-22T15:23:59.633258Z","iopub.execute_input":"2023-10-22T15:23:59.633664Z","iopub.status.idle":"2023-10-22T15:23:59.827828Z","shell.execute_reply.started":"2023-10-22T15:23:59.633629Z","shell.execute_reply":"2023-10-22T15:23:59.826305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-success\" style=\"margin-bottom: 20px; font-size: 14px;\">  \n    <b>üìäObservation and Insights:</b>\n    <ul>\n        <li> Median imputation - For numeric features and mode imputation for categorical features are robust strategies that help preserve the underlying distribution of the data. When you replace missing values with the median for numeric data, you're essentially filling in the gaps with a value that represents the central tendency of the feature, which helps maintain the overall shape of the distribution.</li>\n<li> Mode Imputation - For categorical features, using the mode ensures that you impute missing values with the most frequent category, which is often a reasonable approximation for the missing data while keeping the distribution of the categorical variable intact.</li>\n        <li>These methods are less sensitive to outliers compared to other imputation methods (e.g., mean imputation for numeric data), which is particularly important when dealing with datasets that might contain extreme values that could distort imputed values.</li>\n    </ul>\n</div>","metadata":{"execution":{"iopub.status.busy":"2023-10-19T13:07:02.170010Z","iopub.execute_input":"2023-10-19T13:07:02.170359Z","iopub.status.idle":"2023-10-19T13:07:02.197947Z","shell.execute_reply.started":"2023-10-19T13:07:02.170333Z","shell.execute_reply":"2023-10-19T13:07:02.196957Z"}}},{"cell_type":"markdown","source":"<h2 align=\"left\"><font color=#0E9D00><u>2. Outlier Treatment</u></font></h2>","metadata":{}},{"cell_type":"code","source":"Normal_Features = ['MinTemp','MaxTemp','Humidity3pm','Pressure9am','Pressure3pm','Temp9am','Temp3pm']\nNon_Normal_Features = ['WindGustSpeed','WindSpeed9am','WindSpeed3pm','Humidity9am']","metadata":{"execution":{"iopub.status.busy":"2023-10-22T15:23:59.829339Z","iopub.execute_input":"2023-10-22T15:23:59.829905Z","iopub.status.idle":"2023-10-22T15:23:59.834781Z","shell.execute_reply.started":"2023-10-22T15:23:59.829865Z","shell.execute_reply":"2023-10-22T15:23:59.833860Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"z_score_threshold = 3\n\ndef replace_outliers_with_median(dataframe):\n    for column in Normal_Features:\n        z_scores = np.abs((dataframe[column] - dataframe[column].mean()) / dataframe[column].std())\n        outliers = z_scores > z_score_threshold\n        median_value = dataframe[column].median()\n        dataframe.loc[outliers, column] = median_value\n\n# Detect and replace outliers with the median\nreplace_outliers_with_median(X_train)\nreplace_outliers_with_median(X_test)","metadata":{"execution":{"iopub.status.busy":"2023-10-22T15:23:59.836014Z","iopub.execute_input":"2023-10-22T15:23:59.837099Z","iopub.status.idle":"2023-10-22T15:23:59.913302Z","shell.execute_reply.started":"2023-10-22T15:23:59.837054Z","shell.execute_reply":"2023-10-22T15:23:59.912215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def replace_outliers_with_median_iqr(dataframe):\n    for column in Non_Normal_Features:\n        Q1 = dataframe[column].quantile(0.25)\n        Q3 = dataframe[column].quantile(0.75)\n        IQR = Q3 - Q1\n        lower_bound = Q1 - 1.5 * IQR\n        upper_bound = Q3 + 1.5 * IQR\n        outliers = (dataframe[column] < lower_bound) | (dataframe[column] > upper_bound)\n        median_value = dataframe[column].median()\n        dataframe.loc[outliers, column] = median_value\n\n# Detect and replace outliers with the median using the IQR method\nreplace_outliers_with_median_iqr(X_train)\nreplace_outliers_with_median_iqr(X_test)","metadata":{"execution":{"iopub.status.busy":"2023-10-22T15:23:59.915735Z","iopub.execute_input":"2023-10-22T15:23:59.916289Z","iopub.status.idle":"2023-10-22T15:23:59.983151Z","shell.execute_reply.started":"2023-10-22T15:23:59.916240Z","shell.execute_reply":"2023-10-22T15:23:59.981865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-success\" style=\"margin-bottom: 20px; font-size: 14px;\">  \n    <b>üìäObservation and Insights:</b>\n    <ul>\n        <li> The purpose of selecting the z-score method for normally distributed data and the IQR method for non-normally distributed data as it aligns with the chosen outlier treatment method and the inherent properties of the data, leading to more accurate and robust results. It reflects a data-driven and statistically sound approach to handling outliers in different types of distributions.</li>\n    </ul>\n</div>","metadata":{"execution":{"iopub.status.busy":"2023-10-19T13:07:02.170010Z","iopub.execute_input":"2023-10-19T13:07:02.170359Z","iopub.status.idle":"2023-10-19T13:07:02.197947Z","shell.execute_reply.started":"2023-10-19T13:07:02.170333Z","shell.execute_reply":"2023-10-19T13:07:02.196957Z"}}},{"cell_type":"markdown","source":"<h2 align=\"left\"><font color=#0E9D00><u>3. Encoding</u></font></h2>","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n# Target Variable Encoding\nle = LabelEncoder()\ny_train['RainTomorrow'] = le.fit_transform(y_train['RainTomorrow'])\ny_test['RainTomorrow'] = le.fit_transform(y_test['RainTomorrow'])\n\n# Default Variable Encoding\nX_train['RainToday'] = le.fit_transform(X_train['RainToday'])\nX_test['RainToday'] = le.fit_transform(X_test['RainToday'])","metadata":{"execution":{"iopub.status.busy":"2023-10-22T15:23:59.985318Z","iopub.execute_input":"2023-10-22T15:23:59.985876Z","iopub.status.idle":"2023-10-22T15:24:00.106180Z","shell.execute_reply.started":"2023-10-22T15:23:59.985832Z","shell.execute_reply":"2023-10-22T15:24:00.104809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import category_encoders as ce\n# Initialize the TargetEncoder\nencoder = ce.TargetEncoder(cols=['Location'])\n\n# Fit and transform the encoder on your DataFrame\nX_train = encoder.fit_transform(X_train, y_train['RainTomorrow'])\nX_test = encoder.fit_transform(X_test, y_test['RainTomorrow'])","metadata":{"execution":{"iopub.status.busy":"2023-10-22T15:24:00.108406Z","iopub.execute_input":"2023-10-22T15:24:00.109029Z","iopub.status.idle":"2023-10-22T15:24:01.178317Z","shell.execute_reply.started":"2023-10-22T15:24:00.108988Z","shell.execute_reply":"2023-10-22T15:24:01.176903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# One-Hot Encoding (k-1) Pandas Method\nX_train = pd.concat([X_train, pd.get_dummies(X_train[['WindGustDir','WindDir9am','WindDir3pm']],prefix=['WGD','WD9','WD3'],drop_first=True, dtype=int)],axis=1)\nX_test = pd.concat([X_test, pd.get_dummies(X_test[['WindGustDir','WindDir9am','WindDir3pm']],prefix=['WGD','WD9','WD3'],drop_first=True, dtype=int)], axis=1)\n\n# Now Drop the Original Column - WindGustDir\nX_train = X_train.drop(['WindGustDir','WindDir9am','WindDir3pm'],axis=1)\nX_test = X_test.drop(['WindGustDir','WindDir9am','WindDir3pm'],axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-10-22T15:24:01.180020Z","iopub.execute_input":"2023-10-22T15:24:01.180997Z","iopub.status.idle":"2023-10-22T15:24:01.389007Z","shell.execute_reply.started":"2023-10-22T15:24:01.180953Z","shell.execute_reply":"2023-10-22T15:24:01.387617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-success\" style=\"margin-bottom: 20px; font-size: 14px;\">  \n    <b>üìäObservation and Insights:</b>\n    <ul>\n        <li>All categorical data in the dataset can be classified as nominal data. </li>\n        <li>RainToday, RainTomorrow - Features with binary variable - Encoded with Label Encoding</li>\n<li> Location feature -  70 Unique Values, One-hot Encoding will increasly the dimension and then leading to over-fitting, and hence Target Encoding(Mean of Target Variable) best suitable method here. </li>\n        <li>WindGustDir,WindDir9am,WindDir3pm - 16 Unique Values - One-hot Encoding (k-1) is adopted in this case.</li>\n    </ul>\n</div>","metadata":{"execution":{"iopub.status.busy":"2023-10-19T13:07:02.170010Z","iopub.execute_input":"2023-10-19T13:07:02.170359Z","iopub.status.idle":"2023-10-19T13:07:02.197947Z","shell.execute_reply.started":"2023-10-19T13:07:02.170333Z","shell.execute_reply":"2023-10-19T13:07:02.196957Z"}}},{"cell_type":"markdown","source":"<div style=\"color:white;\n           display:fill;\n           border-radius:20px;\n           background-color:#00F180;\n           font-size:200%;\n           font-family:Verdana;\n           letter-spacing:1px\">\n<a class=\"anchor\" id=\"CH-5\"></a> \n<p style=\"padding: 10px;\n              color:white;\">\n              5. Decision Tree\n</p>\n</div>","metadata":{}},{"cell_type":"code","source":"# Essential's \nfrom sklearn.metrics import accuracy_score,roc_auc_score,f1_score,precision_score,recall_score,confusion_matrix,classification_report,roc_curve,auc\nfrom sklearn.model_selection import GridSearchCV\n\n# Defining a DataFrame to Store all the Significant Results\nresults_df = pd.DataFrame(columns=['Algorithm', 'Accuracy', 'Precision', 'F1 Score', 'ROC-AUC'])","metadata":{"execution":{"iopub.status.busy":"2023-10-22T15:24:01.391817Z","iopub.execute_input":"2023-10-22T15:24:01.392304Z","iopub.status.idle":"2023-10-22T15:24:01.400910Z","shell.execute_reply.started":"2023-10-22T15:24:01.392264Z","shell.execute_reply":"2023-10-22T15:24:01.399608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 align=\"left\"><font color=#0E9D00><u>Creating a Function for Evaluation</u></font></h2>","metadata":{}},{"cell_type":"code","source":"def evaluate_classification(y_train, y_train_pred, y_test, y_test_pred,algorithm_name):\n    global results_df\n    # Calculate and print accuracy scores for training and testing data\n    train_accuracy = accuracy_score(y_train, y_train_pred)\n    test_accuracy = accuracy_score(y_test, y_test_pred)\n    print(\"Accuracy Train:\", train_accuracy)\n    print(\"Accuracy Test:\", test_accuracy)\n    \n    # Generate and print the classification report\n    print(\"Classification Report:\\n\", classification_report(y_test, y_test_pred))\n    \n    \n    # Generate, plot, and display the confusion matrix\n    cnf_matrix = confusion_matrix(y_test, y_test_pred)\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\", fmt='g')\n    plt.title('Confusion Matrix', y=1.1)\n    plt.ylabel('Actual label')\n    plt.xlabel('Predicted label')\n    plt.show()\n    \n    # Generate, plot, and display the ROC curve\n    fpr, tpr, thresholds = roc_curve(y_test, y_test_pred)\n    roc_auc = auc(fpr, tpr)\n    \n    plt.figure(figsize=(8, 6))\n    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n    plt.plot([0, 1], [0, 1], 'r--')\n    plt.title('Receiver Operating Characteristic', fontsize=15)\n    plt.ylabel('True Positive Rate', fontsize=12)\n    plt.xlabel('False Positive Rate', fontsize=12)\n    plt.legend(loc = 'lower right', prop={'size': 12})\n    plt.show()\n    \n    # Calculate accuracy, precision, and F1 score\n    accuracy = accuracy_score(y_test, y_test_pred)\n    precision = precision_score(y_test, y_test_pred)\n    f1 = f1_score(y_test, y_test_pred)\n    \n    # Create a temporary DataFrame with the results\n    temp_df = pd.DataFrame({'Algorithm': [algorithm_name], 'Accuracy': [accuracy], \n                            'Precision': [precision], 'F1 Score': [f1], 'ROC-AUC': [roc_auc]})\n    \n    # Append results to the DataFrame\n    results_df = pd.concat([results_df, temp_df], ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2023-10-22T15:24:01.403093Z","iopub.execute_input":"2023-10-22T15:24:01.403517Z","iopub.status.idle":"2023-10-22T15:24:01.419995Z","shell.execute_reply.started":"2023-10-22T15:24:01.403484Z","shell.execute_reply":"2023-10-22T15:24:01.418641Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 align=\"left\"><font color=#0E9D00><u>Building Model</u></font></h2>","metadata":{}},{"cell_type":"code","source":"%%time\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV\n\n\n# Define the parameter grid\nparam_grid = {\n    'criterion': ['gini', 'entropy'],\n    'splitter': ['best', 'random'],\n    'max_depth': [ 10, 15, 20,25]\n}\n\n# Create the Decision Tree Classifier\ndt_classifier = DecisionTreeClassifier(random_state=42)\n\n# Perform grid search with cross-validation\ngrid_search = GridSearchCV(dt_classifier, param_grid, cv=5, scoring='accuracy',verbose=1, n_jobs=-1)\n\n# Fit the grid search to find the best hyperparameters\ngrid_search.fit(X_train, y_train)\n\n# Get the best hyperparameters\nbest_params = grid_search.best_params_\nbest_score = grid_search.best_score_\n\nprint(\"Best Hyperparameters:\", best_params)\nprint(\"Best Accuracy Score:\", best_score)","metadata":{"execution":{"iopub.status.busy":"2023-10-22T15:24:01.421812Z","iopub.execute_input":"2023-10-22T15:24:01.422493Z","iopub.status.idle":"2023-10-22T15:24:56.593492Z","shell.execute_reply.started":"2023-10-22T15:24:01.422449Z","shell.execute_reply":"2023-10-22T15:24:56.591618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train a Decision Tree Classifier with the best hyperparameters\nbest_dt_classifier = DecisionTreeClassifier(criterion= 'gini',max_depth= 10, splitter= 'best')\nbest_dt_classifier.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2023-10-22T15:24:56.604471Z","iopub.execute_input":"2023-10-22T15:24:56.604999Z","iopub.status.idle":"2023-10-22T15:24:58.425382Z","shell.execute_reply.started":"2023-10-22T15:24:56.604951Z","shell.execute_reply":"2023-10-22T15:24:58.423898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predition of Rainfall and Stroing the Outputs\ny_train_pred = best_dt_classifier.predict(X_train)\ny_test_pred = best_dt_classifier.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2023-10-22T15:24:58.427772Z","iopub.execute_input":"2023-10-22T15:24:58.428330Z","iopub.status.idle":"2023-10-22T15:24:58.521072Z","shell.execute_reply.started":"2023-10-22T15:24:58.428278Z","shell.execute_reply":"2023-10-22T15:24:58.519482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_classification(y_train, y_train_pred, y_test, y_test_pred,'Decision Tree')","metadata":{"execution":{"iopub.status.busy":"2023-10-22T15:24:58.522779Z","iopub.execute_input":"2023-10-22T15:24:58.523192Z","iopub.status.idle":"2023-10-22T15:24:59.491869Z","shell.execute_reply.started":"2023-10-22T15:24:58.523157Z","shell.execute_reply":"2023-10-22T15:24:59.490322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-success\" style=\"margin-bottom: 20px; font-size: 14px;\">  \n    <b>Decision Tree Results:</b>\n    <ul>\n        <b>Accuracy</b>\n        <li>On the training data, the model has an accuracy of approximately 85.99%.</li>\n        <li>On the testing data, the model has an accuracy of about 83.27%. The model is overfitting.</li>\n        <b>AUC-ROC Score</b>\n        <li>An AUC-ROC score of 0.70 suggests a moderate level of discrimination</li>\n        <b>Classification Report</b>\n        <li> Precison - 0.69 - This is crucial if false alarms for rain (predicting rain when it doesn't occur) are costly or have significant consequences. For example, in agriculture, unnecessary irrigation due to false rain predictions can waste resources.</li>\n        <li> Recall - 0.45 - This is important in scenarios where missing a positive prediction (e.g., not predicting rain when it does rain) has significant consequences. In meteorology or disaster management, missing a rain prediction can be problematic.</li>\n        <b>Confusion Matrix</b>\n        <li>The confusion matrix shows that the model correctly predicted 31,198 instances of class 0 (no rain) and 4,325 instances of class 1 (rain). However, it also made 1,926 false positive predictions for class 1 and 5,209 false negatives for class 0.</li>\n        <b>Wall Time</b>\n        <li>The model took 55.2 seconds to train and test. Wall time can be an important consideration, especially in real-time or time-sensitive applications.</li>\n    </ul>\n</div>","metadata":{"execution":{"iopub.status.busy":"2023-10-19T13:07:02.170010Z","iopub.execute_input":"2023-10-19T13:07:02.170359Z","iopub.status.idle":"2023-10-19T13:07:02.197947Z","shell.execute_reply.started":"2023-10-19T13:07:02.170333Z","shell.execute_reply":"2023-10-19T13:07:02.196957Z"}}},{"cell_type":"markdown","source":"<div style=\"color:white;\n           display:fill;\n           border-radius:20px;\n           background-color:#00F180;\n           font-size:200%;\n           font-family:Verdana;\n           letter-spacing:1px\">\n<a class=\"anchor\" id=\"CH-6\"></a> \n<p style=\"padding: 10px;\n              color:white;\">\n              6. Random Forest\n</p>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<h2 align=\"left\"><font color=#0E9D00><u>Building Model</u></font></h2>","metadata":{}},{"cell_type":"code","source":"%%time\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Define the parameter grid\nparam_grid = {\n    'n_estimators': [20,40,50],\n    'criterion': ['gini', 'entropy'],\n    'max_depth': [ 10,15, 20, 25, 30]\n}\n\n# Create the Random Forest Classifier\nrf_classifier = RandomForestClassifier(random_state=42)\n\n# Perform grid search with cross-validation\ngrid_search = GridSearchCV(rf_classifier, param_grid, cv=5, scoring='accuracy',verbose=1, n_jobs=-1)\n\n# Fit the grid search to find the best hyperparameters\ngrid_search.fit(X_train, y_train.values.ravel())\n\n# Get the best hyperparameters\nbest_params = grid_search.best_params_\nbest_score = grid_search.best_score_\n\nprint(\"Best Hyperparameters:\", best_params)\nprint(\"Best Accuracy Score:\", best_score)","metadata":{"execution":{"iopub.status.busy":"2023-10-22T15:24:59.493696Z","iopub.execute_input":"2023-10-22T15:24:59.494798Z","iopub.status.idle":"2023-10-22T15:33:02.972078Z","shell.execute_reply.started":"2023-10-22T15:24:59.494742Z","shell.execute_reply":"2023-10-22T15:33:02.970668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train a Random Forest Classifier with the best hyperparameters\nbest_rf_classifier = RandomForestClassifier(criterion= 'gini', max_depth= 25, n_estimators = 50)\nbest_rf_classifier.fit(X_train, y_train.values.ravel())","metadata":{"execution":{"iopub.status.busy":"2023-10-22T15:33:02.974440Z","iopub.execute_input":"2023-10-22T15:33:02.975024Z","iopub.status.idle":"2023-10-22T15:33:19.085271Z","shell.execute_reply.started":"2023-10-22T15:33:02.974967Z","shell.execute_reply":"2023-10-22T15:33:19.083906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predition of Rainfall and Stroing the Outputs\ny_train_pred = best_rf_classifier.predict(X_train)\ny_test_pred = best_rf_classifier.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2023-10-22T15:33:19.087727Z","iopub.execute_input":"2023-10-22T15:33:19.089063Z","iopub.status.idle":"2023-10-22T15:33:21.767195Z","shell.execute_reply.started":"2023-10-22T15:33:19.089019Z","shell.execute_reply":"2023-10-22T15:33:21.765885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 align=\"left\"><font color=#0E9D00><u>Model Evaluation</u></font></h2>","metadata":{}},{"cell_type":"code","source":"evaluate_classification(y_train, y_train_pred, y_test, y_test_pred,'Random Forest')","metadata":{"execution":{"iopub.status.busy":"2023-10-22T15:33:21.769175Z","iopub.execute_input":"2023-10-22T15:33:21.769765Z","iopub.status.idle":"2023-10-22T15:33:22.541905Z","shell.execute_reply.started":"2023-10-22T15:33:21.769714Z","shell.execute_reply":"2023-10-22T15:33:22.540644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-success\" style=\"margin-bottom: 20px; font-size: 14px;\">  \n    <b>Random Forest Results:</b>\n    <ul>\n        <b>Accuracy</b>\n        <li>On the training data, the Random Forest model has a high accuracy of approximately 98.53%.</li>\n        <li>On the testing data, the accuracy is also quite good, at about 84.77%. The model is overfitting.</li>\n        <b>AUC-ROC Score</b>\n        <li>The AUC-ROC score is 0.72. It suggests that the Random Forest model has a moderate ability to discriminate between positive and negative classes.</li>\n        <b>Classification Report</b>\n    <li>Precision for class 0 (no rain) is 0.86, indicating that when the model predicts no rain, it's correct about 86% of the time.</li>\n    <li>Precision for class 1 (rain) is 0.75, suggesting that when the model predicts rain, it's correct about 75% of the time.</li>\n    <li>Recall for class 0 is 0.95, which means that the model correctly captures 95% of the actual instances of no rain.</li>\n    <li>Recall for class 1 is 0.48, indicating that the model captures only 48% of actual instances of rain.</li>\n        <b>Confusion Matrix</b>\n        <li>The confusion matrix shows that the model correctly predicted 31,579 instances of class 0 and 4,583 instances of class 1. It also made 1,545 false positive predictions for class 1 and 4,951 false negatives for class 0.\n</li>\n        <b>Wall Time</b>\n        <li>The model took 8 minutes and 3 seconds to train and test. Random Forest models can be more computationally intensive, but they often provide good predictive performance.</li>\n        <li>The Random Forest model shows a strong performance in terms of accuracy, precision for class 0 (no rain), and F1-score for class 0. However, the recall for class 1 (rain) is relatively low, indicating that the model struggles to correctly capture instances of rain. </li>\n    </ul>\n</div>","metadata":{"execution":{"iopub.status.busy":"2023-10-19T13:07:02.170010Z","iopub.execute_input":"2023-10-19T13:07:02.170359Z","iopub.status.idle":"2023-10-19T13:07:02.197947Z","shell.execute_reply.started":"2023-10-19T13:07:02.170333Z","shell.execute_reply":"2023-10-19T13:07:02.196957Z"}}},{"cell_type":"markdown","source":"<div style=\"color:white;\n           display:fill;\n           border-radius:20px;\n           background-color:#00F180;\n           font-size:200%;\n           font-family:Verdana;\n           letter-spacing:1px\">\n<a class=\"anchor\" id=\"CH-7\"></a> \n<p style=\"padding: 10px;\n              color:white;\">\n              7. Ada Boost\n</p>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<h2 align=\"left\"><font color=#0E9D00><u>Building Model</u></font></h2>","metadata":{}},{"cell_type":"code","source":"%%time\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Define the parameter grid\nparam_grid = {\n    'estimator': [DecisionTreeClassifier(max_depth=1), DecisionTreeClassifier(max_depth=2)],\n    'n_estimators': [20,40,50],\n    'learning_rate': [0.01,0.1, 0.5, 1.0,5]\n}\n\n# Create the AdaBoost Classifier\nada_classifier = AdaBoostClassifier(random_state=42)\n\n# Perform grid search with cross-validation\ngrid_search = GridSearchCV(ada_classifier, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n\n# Fit the grid search to find the best hyperparameters\ngrid_search.fit(X_train, y_train.values.ravel())\n\n# Get the best hyperparameters\nbest_params = grid_search.best_params_\nbest_score = grid_search.best_score_\n\nprint(\"Best Hyperparameters:\", best_params)\nprint(\"Best Accuracy Score:\", best_score)","metadata":{"execution":{"iopub.status.busy":"2023-10-22T15:33:22.543575Z","iopub.execute_input":"2023-10-22T15:33:22.543994Z","iopub.status.idle":"2023-10-22T15:41:30.707070Z","shell.execute_reply.started":"2023-10-22T15:33:22.543943Z","shell.execute_reply":"2023-10-22T15:41:30.705640Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_ada_classifier = AdaBoostClassifier(estimator= DecisionTreeClassifier(max_depth=2), learning_rate= 1.0, n_estimators= 50)\nbest_ada_classifier.fit(X_train, y_train.values.ravel())","metadata":{"execution":{"iopub.status.busy":"2023-10-22T15:41:30.708624Z","iopub.execute_input":"2023-10-22T15:41:30.709563Z","iopub.status.idle":"2023-10-22T15:41:48.638807Z","shell.execute_reply.started":"2023-10-22T15:41:30.709509Z","shell.execute_reply":"2023-10-22T15:41:48.637468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train_pred = best_ada_classifier.predict(X_train)\ny_test_pred = best_ada_classifier.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2023-10-22T15:41:48.641003Z","iopub.execute_input":"2023-10-22T15:41:48.641975Z","iopub.status.idle":"2023-10-22T15:41:50.270618Z","shell.execute_reply.started":"2023-10-22T15:41:48.641925Z","shell.execute_reply":"2023-10-22T15:41:50.269200Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 align=\"left\"><font color=#0E9D00><u>Model Evaluation</u></font></h2>","metadata":{}},{"cell_type":"code","source":"evaluate_classification(y_train, y_train_pred, y_test, y_test_pred,'Ada Boost')","metadata":{"execution":{"iopub.status.busy":"2023-10-22T15:41:50.272175Z","iopub.execute_input":"2023-10-22T15:41:50.273521Z","iopub.status.idle":"2023-10-22T15:41:51.122601Z","shell.execute_reply.started":"2023-10-22T15:41:50.273477Z","shell.execute_reply":"2023-10-22T15:41:51.121114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-success\" style=\"margin-bottom: 20px; font-size: 14px;\">  \n    <b>Ada Boost Results:</b>\n    <ul>\n        <b>Accuracy</b>\n        <li>On the training data, the Random Forest model has a high accuracy of approximately 98.53%.</li>\n        <li>On the testing data, the accuracy is also quite good, at about 84.77%. The model is overfitting.</li>\n        <b>AUC-ROC Score</b>\n        <li>The AUC-ROC score is 0.72. It suggests that the Random Forest model has a moderate ability to discriminate between positive and negative classes.</li>\n        <b>Classification Report</b>\n    <li>Precision for class 0 (no rain) is 0.86, indicating that when the model predicts no rain, it's correct about 86% of the time.</li>\n    <li>Precision for class 1 (rain) is 0.75, suggesting that when the model predicts rain, it's correct about 75% of the time.</li>\n    <li>Recall for class 0 is 0.95, which means that the model correctly captures 95% of the actual instances of no rain.</li>\n    <li>Recall for class 1 is 0.48, indicating that the model captures only 48% of actual instances of rain.</li>\n        <b>Confusion Matrix</b>\n        <li>The confusion matrix shows that the model correctly predicted 31,579 instances of class 0 and 4,583 instances of class 1. It also made 1,545 false positive predictions for class 1 and 4,951 false negatives for class 0.\n</li>\n        <b>Wall Time</b>\n        <li>The model took 8 minutes and 3 seconds to train and test. Random Forest models can be more computationally intensive, but they often provide good predictive performance.</li>\n        <li>The Random Forest model shows a strong performance in terms of accuracy, precision for class 0 (no rain), and F1-score for class 0. However, the recall for class 1 (rain) is relatively low, indicating that the model struggles to correctly capture instances of rain. </li>\n    </ul>\n</div>","metadata":{"execution":{"iopub.status.busy":"2023-10-19T13:07:02.170010Z","iopub.execute_input":"2023-10-19T13:07:02.170359Z","iopub.status.idle":"2023-10-19T13:07:02.197947Z","shell.execute_reply.started":"2023-10-19T13:07:02.170333Z","shell.execute_reply":"2023-10-19T13:07:02.196957Z"}}},{"cell_type":"markdown","source":"<div style=\"color:white;\n           display:fill;\n           border-radius:20px;\n           background-color:#00F180;\n           font-size:200%;\n           font-family:Verdana;\n           letter-spacing:1px\">\n<a class=\"anchor\" id=\"CH-8\"></a> \n<p style=\"padding: 10px;\n              color:white;\">\n              8. Gradient Boost\n</p>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<h2 align=\"left\"><font color=#0E9D00><u>Building Model</u></font></h2>","metadata":{}},{"cell_type":"code","source":"%%time\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n# Define the parameter grid\nparam_grid = {\n    'n_estimators': [25,40,50],\n    'learning_rate': [0.01, 0.1, 0.5, 1.0,5],\n    'max_depth': [3, 5]\n}\n\n# Create the Gradient Boosting Classifier\ngb_classifier = GradientBoostingClassifier(random_state=42)\n\n# Perform grid search with cross-validation\ngrid_search = GridSearchCV(gb_classifier, param_grid, cv=5, scoring='accuracy',verbose=1, n_jobs=-1)\n\n# Fit the grid search to find the best hyperparameters\ngrid_search.fit(X_train, y_train.values.ravel())\n\n# Get the best hyperparameters\nbest_params = grid_search.best_params_\nbest_score = grid_search.best_score_\n\nprint(\"Best Hyperparameters:\", best_params)\nprint(\"Best Accuracy Score:\", best_score)","metadata":{"execution":{"iopub.status.busy":"2023-10-22T15:41:51.124105Z","iopub.execute_input":"2023-10-22T15:41:51.124492Z","iopub.status.idle":"2023-10-22T15:56:33.088605Z","shell.execute_reply.started":"2023-10-22T15:41:51.124457Z","shell.execute_reply":"2023-10-22T15:56:33.087151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_gb_classifier = GradientBoostingClassifier(learning_rate= 0.5, max_depth= 5, n_estimators= 50)\nbest_gb_classifier.fit(X_train, y_train.values.ravel())","metadata":{"execution":{"iopub.status.busy":"2023-10-22T15:56:33.091651Z","iopub.execute_input":"2023-10-22T15:56:33.092134Z","iopub.status.idle":"2023-10-22T15:57:01.244883Z","shell.execute_reply.started":"2023-10-22T15:56:33.092094Z","shell.execute_reply":"2023-10-22T15:57:01.243888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train_pred = best_gb_classifier.predict(X_train)\ny_test_pred = best_gb_classifier.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2023-10-22T15:57:01.246446Z","iopub.execute_input":"2023-10-22T15:57:01.246869Z","iopub.status.idle":"2023-10-22T15:57:01.615669Z","shell.execute_reply.started":"2023-10-22T15:57:01.246833Z","shell.execute_reply":"2023-10-22T15:57:01.614375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 align=\"left\"><font color=#0E9D00><u>Model Evaluation</u></font></h2>","metadata":{}},{"cell_type":"code","source":"evaluate_classification(y_train, y_train_pred, y_test, y_test_pred,'Gradient Boost')","metadata":{"execution":{"iopub.status.busy":"2023-10-22T15:57:01.617294Z","iopub.execute_input":"2023-10-22T15:57:01.617720Z","iopub.status.idle":"2023-10-22T15:57:02.509258Z","shell.execute_reply.started":"2023-10-22T15:57:01.617681Z","shell.execute_reply":"2023-10-22T15:57:02.507814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-success\" style=\"margin-bottom: 20px; font-size: 14px;\">  \n<b>Gradient Boost Model Results:</b>\n\n<b>Accuracy:</b>\n- On the training data, the Gradient Boost model achieved an accuracy of approximately 87.10%.\n- On the testing data, the accuracy was approximately 84.60%.\n\n<b>AUC-ROC Score:</b>\n- The AUC-ROC score is 0.73, indicating a moderate to good ability of the Gradient Boost model to discriminate between positive and negative classes.\n\n<b>Classification Report:</b>\n- Precision for class 0 (no rain) is 0.87, suggesting that when the model predicts no rain, it's correct about 87% of the time.\n- Precision for class 1 (rain) is 0.72, indicating that when the model predicts rain, it's correct about 72% of the time.\n- Recall for class 0 is 0.94, meaning that the model correctly captures 94% of the actual instances of no rain.\n- Recall for class 1 is 0.51, implying that the model captures 51% of actual instances of rain.\n\n<b>Confusion Matrix:</b>\n- The confusion matrix shows that the model correctly predicted 31,201 instances of class 0 and 4,887 instances of class 1. However, it also made 1,923 false positive predictions for class 1 and 4,647 false negatives for class 0.\n\n<b>Wall Time:</b>\n- The model took 14 minutes and 41 seconds to train and test.\n\n","metadata":{"execution":{"iopub.status.busy":"2023-10-19T13:07:02.170010Z","iopub.execute_input":"2023-10-19T13:07:02.170359Z","iopub.status.idle":"2023-10-19T13:07:02.197947Z","shell.execute_reply.started":"2023-10-19T13:07:02.170333Z","shell.execute_reply":"2023-10-19T13:07:02.196957Z"}}},{"cell_type":"markdown","source":"<div style=\"color:white;\n           display:fill;\n           border-radius:20px;\n           background-color:#00F180;\n           font-size:200%;\n           font-family:Verdana;\n           letter-spacing:1px\">\n<a class=\"anchor\" id=\"CH-9\"></a> \n<p style=\"padding: 10px;\n              color:white;\">\n              9. XGBoost\n</p>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<h2 align=\"left\"><font color=#0E9D00><u>Building Model</u></font></h2>","metadata":{}},{"cell_type":"code","source":"%%time\nimport xgboost as xgb\n\n# Define the parameter grid\nparam_grid = {\n    'n_estimators': [25,40,50],\n    'learning_rate': [0.01, 0.1, 0.5, 1.0],\n    'max_depth': [3, 5]\n}\n\n# Create the XGBoost Classifier\nxgb_classifier = xgb.XGBClassifier(random_state=42)\n\n# Perform grid search with cross-validation\ngrid_search = GridSearchCV(xgb_classifier, param_grid, cv=5, scoring='accuracy',verbose=1, n_jobs=-1)\n\n# Fit the grid search to find the best hyperparameters\ngrid_search.fit(X_train, y_train)\n\n# Get the best hyperparameters\nbest_params = grid_search.best_params_\nbest_score = grid_search.best_score_\n\nprint(\"Best Hyperparameters:\", best_params)\nprint(\"Best Accuracy Score:\", best_score)","metadata":{"execution":{"iopub.status.busy":"2023-10-22T15:57:02.511379Z","iopub.execute_input":"2023-10-22T15:57:02.511947Z","iopub.status.idle":"2023-10-22T16:03:39.947243Z","shell.execute_reply.started":"2023-10-22T15:57:02.511896Z","shell.execute_reply":"2023-10-22T16:03:39.945867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_xgb_classifier = xgb.XGBClassifier(learning_rate = 0.5, max_depth= 5, n_estimators= 50)\nbest_xgb_classifier.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2023-10-22T16:03:39.948931Z","iopub.execute_input":"2023-10-22T16:03:39.949336Z","iopub.status.idle":"2023-10-22T16:03:46.457968Z","shell.execute_reply.started":"2023-10-22T16:03:39.949301Z","shell.execute_reply":"2023-10-22T16:03:46.456656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train_pred = best_xgb_classifier.predict(X_train)\ny_test_pred = best_xgb_classifier.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2023-10-22T16:03:46.459535Z","iopub.execute_input":"2023-10-22T16:03:46.459965Z","iopub.status.idle":"2023-10-22T16:03:46.647139Z","shell.execute_reply.started":"2023-10-22T16:03:46.459930Z","shell.execute_reply":"2023-10-22T16:03:46.646014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 align=\"left\"><font color=#0E9D00><u>Model Evaluation</u></font></h2>","metadata":{}},{"cell_type":"code","source":"evaluate_classification(y_train, y_train_pred, y_test, y_test_pred,'XGBoost')","metadata":{"execution":{"iopub.status.busy":"2023-10-22T16:03:46.648796Z","iopub.execute_input":"2023-10-22T16:03:46.649443Z","iopub.status.idle":"2023-10-22T16:03:47.578563Z","shell.execute_reply.started":"2023-10-22T16:03:46.649406Z","shell.execute_reply":"2023-10-22T16:03:47.577195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-success\" style=\"margin-bottom: 20px; font-size: 14px;\">  \n<b>XG Boost Model Results:</b>\n\n<b>Accuracy:</b>\n- On the training data, the XG Boost model achieved an accuracy of approximately 86.66%.\n- On the testing data, the accuracy was approximately 85.02%.\n\n<b>AUC-ROC Score:</b>\n- The AUC-ROC score is 0.73, indicating a moderate to good ability of the XG Boost model to discriminate between positive and negative classes.\n\n<b>Classification Report:</b>\n- Precision for class 0 (no rain) is 0.87, suggesting that when the model predicts no rain, it's correct about 87% of the time.\n- Precision for class 1 (rain) is 0.74, indicating that when the model predicts rain, it's correct about 74% of the time.\n- Recall for class 0 is 0.95, meaning that the model correctly captures 95% of the actual instances of no rain.\n- Recall for class 1 is 0.51, implying that the model captures 51% of actual instances of rain.\n- The F1-score for class 0 is 0.91, while for class 1, it is 0.61.\n\n<b>Confusion Matrix:</b>\n- The confusion matrix shows that the model correctly predicted 31,358 instances of class 0 and 4,909 instances of class 1. However, it also made 1,766 false positive predictions for class 1 and 4,625 false negatives for class 0.\n\n<b>Wall Time:</b>\n- The model took 6 minutes and 37 seconds to train and test.","metadata":{"execution":{"iopub.status.busy":"2023-10-19T13:07:02.170010Z","iopub.execute_input":"2023-10-19T13:07:02.170359Z","iopub.status.idle":"2023-10-19T13:07:02.197947Z","shell.execute_reply.started":"2023-10-19T13:07:02.170333Z","shell.execute_reply":"2023-10-19T13:07:02.196957Z"}}},{"cell_type":"markdown","source":"<div style=\"color:white;\n           display:fill;\n           border-radius:20px;\n           background-color:#00F180;\n           font-size:200%;\n           font-family:Verdana;\n           letter-spacing:1px\">\n<a class=\"anchor\" id=\"CH-10\"></a> \n<p style=\"padding: 10px;\n              color:white;\">\n              10. Result and Discussion üìà‚úîÔ∏è\n</p>\n</div>","metadata":{}},{"cell_type":"code","source":"# Define a function to highlight maximum values\ndef highlight_max_except_first_col(col):\n    if col.name == results_df.columns[0]:\n        return [''] * len(col)\n    is_max = col == col.max()\n    return ['background-color: #00F180' if v else '' for v in is_max]\n\n# Apply the function to the entire DataFrame\nstyled_df = results_df.style.apply(highlight_max_except_first_col)\n\n# Display the styled DataFrame\nstyled_df","metadata":{"execution":{"iopub.status.busy":"2023-10-22T16:03:47.581268Z","iopub.execute_input":"2023-10-22T16:03:47.581864Z","iopub.status.idle":"2023-10-22T16:03:47.707119Z","shell.execute_reply.started":"2023-10-22T16:03:47.581813Z","shell.execute_reply":"2023-10-22T16:03:47.705646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-success\" style=\"margin-bottom: 20px; font-size: 16px;\">  \n    <b>Result and Discussion</b>\n    <ul>\n<li>If prioritize accuracy, Random Forest and XG Boost are the top performers.</li>\n<li>If prioritize AUC-ROC score, Gradient Boost and XG Boost are strong choices.</li>\n<li>For precision and recall in class 0, Random Forest and XG Boost stand out.</li>\n<li>For precision in class 1, XG Boost performs well.</li>\n<li>For recall in class 1, XG Boost and Gradient Boost are competitive.</li>\n<li>If there is need to balance computational efficiency and performance, Decision Tree is the fastest, but Random Forest and XG Boost are also viable options. </li>\n        <li> In this context, XG Boost emerges as the leading choice, excelling in performance. </li>\n</div>","metadata":{"execution":{"iopub.status.busy":"2023-10-19T13:07:02.170010Z","iopub.execute_input":"2023-10-19T13:07:02.170359Z","iopub.status.idle":"2023-10-19T13:07:02.197947Z","shell.execute_reply.started":"2023-10-19T13:07:02.170333Z","shell.execute_reply":"2023-10-19T13:07:02.196957Z"}}}]}